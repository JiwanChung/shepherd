<div align="center">

# ğŸ‘ Shepherd

**A resilient Slurm job orchestrator that keeps your GPU workloads running**

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-3776ab.svg?style=flat&logo=python&logoColor=white)](https://www.python.org)
[![License: MIT](https://img.shields.io/badge/license-MIT-green.svg?style=flat)](LICENSE)
[![Slurm](https://img.shields.io/badge/slurm-compatible-orange.svg?style=flat)](https://slurm.schedmd.com/)
[![Zero Dependencies](https://img.shields.io/badge/dependencies-zero-brightgreen.svg?style=flat)]()

[Features](#features) â€¢ [Installation](#install) â€¢ [Quick Start](#quick-start) â€¢ [Documentation](#how-it-works) â€¢ [Contributing](#tests)

<img src="assets/demo.gif" alt="Shepherd TUI Demo" width="700">

</div>

---

## The Problem

Running ML training jobs on shared Slurm clusters is painful:

- ğŸ”¥ Nodes go down mid-training
- ğŸ’¥ GPUs throw random CUDA errors
- ğŸš§ Partitions get congested
- â° Preemption kills your 3-day run at hour 71

**Shepherd watches your jobs and handles failures automatically** â€” so you can sleep while your models train.

---

## Features

<table>
<tr>
<td width="50%">

### ğŸ”„ Automatic Recovery
- **Heartbeat monitoring** â€” Detects hung jobs
- **GPU fault detection** â€” Catches CUDA errors early
- **Node blacklisting** â€” Avoids problematic nodes
- **Exponential backoff** â€” Smart retry delays

</td>
<td width="50%">

### ğŸ¯ Smart Scheduling
- **Partition auto-discovery** â€” Finds matching GPUs
- **Partition failover** â€” Tries next queue on failure
- **VRAM filtering** â€” 40GB+, 80GB+, etc.
- **Preference modes** â€” Best or cheapest first

</td>
</tr>
<tr>
<td width="50%">

### ğŸ–¥ï¸ Modern Interface
- **Interactive TUI** â€” Beautiful terminal UI
- **Node management** â€” Ban/unban with keyboard
- **GPU smoke tests** â€” Test CUDA on all nodes
- **Log viewer** â€” Tail logs in real-time
- **Status icons** â€” See state at a glance

</td>
<td width="50%">

### ğŸŒ Remote Clusters
- **SSH execution** â€” Run on remote clusters
- **Auto-sync** â€” Code syncs when changed
- **Zero setup** â€” Uses your SSH config
- **No admin required** â€” Standard Slurm CLI only

</td>
</tr>
</table>

---

## Install

```bash
git clone <repo> && cd shepherd
uv tool install .
```

Or run directly from source:

```bash
python -m shepherd --help
```

## Quick Start

Create a script (`train.sh`):

```bash
#!/bin/bash
#SHEPHERD --gpus 4 --min-vram 40

python train.py
```

All `#SHEPHERD` directives (CLI flags override these):

| Directive | Description |
|-----------|-------------|
| `--gpus N` | Minimum GPUs per node |
| `--min-vram N` | Minimum VRAM per GPU (GB) |
| `--max-vram N` | Maximum VRAM per GPU (GB) |
| `--prefer min\|max` | Partition ordering |
| `--mode run_once\|indefinite` | Run mode |
| `--partitions a,b,c` | Manual partition list |
| `--max-retries N` | Max restart attempts |
| `--keep-alive N` | Duration in seconds (indefinite mode) |
| `--heartbeat-interval N` | Heartbeat frequency (seconds) |
| `--heartbeat-grace N` | Grace period before restart (seconds) |
| `--backoff-base N` | Base backoff delay (seconds) |
| `--backoff-max N` | Max backoff delay (seconds) |
| `--blacklist-ttl N` | Node blacklist duration (seconds) |

Run with shepherd:

```bash
shepherd train.sh
shepherd
```

Shepherd parses `#SHEPHERD` directives, auto-discovers matching GPU partitions, and orders them best-first. Use `--prefer min` for cheapest-first. Jobs auto-restart on preemption with bad node blacklisting.

## TUI

A modern terminal interface for monitoring and controlling jobs:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ SHEPHERD â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3 runs Â· 2 running Â· 1 pending                                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â–¸ llama-finetune       â— running    job=284719    2h ago                         â”‚
â”‚    gpt-evaluation       â— running    job=284720    45m ago                        â”‚
â”‚    mistral-pretrain     â— pending    job=284721    5m ago                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ SLURM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ RUN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Job ID      284719                           Heartbeat   5s ago                  â”‚
â”‚  State       RUNNING                          Restarts    2                       â”‚
â”‚  Partition   gpu-a100                         Mode        indefinite              â”‚
â”‚  Node        gpu-node-042                                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â†‘â†“ navigate  Tab panel  r restart  s stop  p pause  q quit                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Bindings

| Key | Action |
|-----|--------|
| `j/k` or arrows | Navigate runs |
| `Tab` | Cycle right panel: INFO â†’ SCRIPT â†’ LOGS |
| `[/]` or `-/=` | Scroll right panel |
| `PgUp/PgDn` | Scroll by page |
| `1/2` | Switch stdout/stderr (in LOGS tab) |
| `r` | Restart selected run |
| `s` | Stop selected run |
| `p/u` | Pause/unpause |
| `n` | Create new run |
| `b` | Manage blacklist |
| `/` | Filter runs |
| `o` | Cycle sort order |
| `Enter` | Fullscreen detail view |
| `?` | Help |
| `q` | Quit |

### Nodes TUI

The nodes view (`shepherd nodes`) provides GPU smoke testing:

| Key | Action |
|-----|--------|
| `â†‘/â†“` | Navigate nodes |
| `Enter` | Ban/unban node |
| `s` | Run CUDA smoke tests on all nodes |
| `r` | Refresh node list |
| `q` | Quit |

Smoke test results:
| Icon | Status |
|------|--------|
| `âœ“` | CUDA working |
| `âœ—` | CUDA failed |
| `â—‹` | No CUDA libs installed |
| `â—` | Node busy (no free GPUs) |
| `âŠ˜` | QOS/partition error |
| `â±` | Timeout |

### GPU Availability

The `shepherd gpus` command shows max assignable GPUs per partition:

```
Partition                GPU            VRAM    Max   Avail  Nodes      Total
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
suma_a100                a100           80GB   8    â—‹ 8    0/3       24/24
gigabyte_A6000           a6000          48GB   8    â—‹ 8    3/6       48/48
suma_rtx4090             rtx4090        24GB   6    â— 4    2/11      66/66
```

| Column | Description |
|--------|-------------|
| Max | Maximum GPUs on a single node |
| Avail | Max available on any single node now |
| Nodes | Nodes with free GPUs / total nodes |
| Total | Cluster-wide available / total GPUs |

Availability icons: `â—‹` (green, full) `â—` (yellow, partial) `â—` (red, none)

### Status Icons

| Icon | Status |
|------|--------|
| `â—` | Healthy running |
| `â—` | Running (degraded/paused) |
| `â—‹` | Pending |
| `â†»` | Restarting |
| `âœ–` | Unresponsive |
| `âœ“` | Completed |
| `â– ` | Stopped |

### Log Viewing

The LOGS tab automatically finds Slurm output files:

1. Explicit path from `meta.json` (`stdout_path` / `stderr_path`)
2. Parsed from sbatch script (`#SBATCH --output=...`)
3. Default Slurm pattern `slurm-<job_id>.out`
4. Fallback to shepherd run directory

## Remote Execution

Run shepherd on a remote Slurm cluster via SSH:

```bash
# Sync code to remote (one-time)
shepherd --remote mycluster sync

# Run TUI on remote (with TTY)
shepherd --remote mycluster tui

# Other commands
shepherd --remote mycluster list
shepherd --remote mycluster status --run-id my-job
shepherd --remote mycluster control restart --run-id my-job
```

The daemon auto-starts on the remote host. Options:

| Flag | Description |
|------|-------------|
| `--remote HOST` | SSH host (from ~/.ssh/config) |
| `--remote-python CMD` | Custom Python command |
| `--remote-dir DIR` | Sync destination (default: ~/.local/lib/shepherd) |

## Partition Fallback

Automatically failover to backup partitions when submission fails:

```json
{
  "run_id": "my-job",
  "run_mode": "run_once",
  "sbatch_script": "~/jobs/train.sh",
  "partition_fallback": {
    "partitions": ["gpu-high", "gpu-low", "cpu"],
    "retry_per_partition": 2,
    "reset_to_preferred_sec": 3600
  }
}
```

| Field | Default | Description |
|-------|---------|-------------|
| `partitions` | required | Ordered list (first = preferred) |
| `retry_per_partition` | 2 | Failures before next partition |
| `reset_to_preferred_sec` | 3600 | Interval to retry preferred |

## How It Works

Shepherd wraps your script with monitoring and manages the full lifecycle:

```
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚      Your Training Job      â”‚
                                    â”‚    (python train.py, etc)   â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                   â”‚
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚     Shepherd Wrapper        â”‚
                                    â”‚  â€¢ GPU visibility check     â”‚
                                    â”‚  â€¢ CUDA smoke test          â”‚
                                    â”‚  â€¢ Heartbeat thread         â”‚
                                    â”‚  â€¢ Failure reporting        â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                      Shepherd Daemon                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚   Monitor   â”‚  â”‚  Heartbeat  â”‚  â”‚   Auto-     â”‚  â”‚  Partition  â”‚  â”‚    Node     â”‚            â”‚
â”‚  â”‚  Job State  â”‚  â”‚ Validation  â”‚  â”‚  Restart    â”‚  â”‚  Failover   â”‚  â”‚ Blacklist   â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                   â”‚
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚           Slurm             â”‚
                                    â”‚   sbatch / squeue / scancel â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**The wrapper provides:**
- **Heartbeat** â€” Detect stuck jobs before wasting hours
- **GPU probes** â€” Fail fast on bad nodes (CUDA test, visibility check)
- **Exit codes** â€” Semantic codes trigger appropriate recovery actions

## CLI Reference

```bash
# Create a run (auto-discovers GPU partitions)
shepherd train.sh                                      # uses #SHEPHERD directives from script
shepherd train.sh --gpus 4 --min-vram 40               # override: 4+ GPUs, 40GB+ VRAM
shepherd train.sh --gpus 4 --min-vram 40 --max-vram 48 # 40-48GB range
shepherd train.sh --prefer min                         # cheapest-first ordering

# Open TUI
shepherd

# List runs
shepherd list

# Check status
shepherd status --run-id my-job

# Control operations
shepherd control pause --run-id my-job
shepherd control unpause --run-id my-job
shepherd control stop --run-id my-job
shepherd control restart --run-id my-job

# Node management
shepherd nodes                                         # interactive TUI (press 's' for smoke tests)
shepherd nodes --list                                  # list all nodes
shepherd nodes ban --node node001 --ttl 3600 --reason "Bad GPU"
shepherd nodes unban --node node001

# GPU availability per partition
shepherd gpus                                          # show max assignable GPUs per partition

# View logs
shepherd logs --run-id my-job                          # stdout (last 50 lines)
shepherd logs --run-id my-job --stderr                 # stderr
shepherd logs --run-id my-job -f                       # follow (tail -f)
shepherd logs --run-id my-job -n 100                   # last 100 lines

# Remote config
shepherd --remote mycluster config list                # show all config
shepherd --remote mycluster config set conda_env base  # set conda env

# Sync code to remote
shepherd --remote mycluster sync                       # sync and restart daemon
shepherd --remote mycluster sync --no-restart          # sync without restart

# TUI
shepherd tui
```

Add `--json` for machine-readable output. All commands support `--remote HOST` for remote execution.

### Additional Flags

| Flag | Description |
|------|-------------|
| `--no-blacklist` | Disable node blacklisting for this run |
| `--no-sync` | Skip auto-sync to remote |
| `--no-daemon` | Skip auto-starting remote daemon |

## Configuration

### meta.json fields

| Field | Type | Description |
|-------|------|-------------|
| `run_id` | string | Unique identifier |
| `run_mode` | string | `run_once` or `indefinite` |
| `sbatch_script` | string | Path to sbatch script |
| `sbatch_args` | string/list | Extra sbatch arguments |
| `max_retries` | int | Max restarts for run_once mode |
| `keep_alive_sec` | int | Duration for indefinite mode |
| `heartbeat_interval_sec` | int | Expected heartbeat interval (default: 30) |
| `heartbeat_grace_sec` | int | Grace period before restart (default: 90) |
| `progress_stall_sec` | int | Max time without progress update |
| `backoff_base_sec` | int | Base backoff delay (default: 10) |
| `backoff_max_sec` | int | Max backoff delay (default: 300) |
| `blacklist_ttl_sec` | int | How long to blacklist bad nodes |
| `blacklist_limit` | int | Max nodes to exclude (default: 64) |
| `partition_fallback` | object | Partition failover config |

## State Directory

All state lives under `~/.slurm_shepherd/`:

```
~/.slurm_shepherd/
â”œâ”€â”€ runs/
â”‚   â””â”€â”€ <run_id>/
â”‚       â”œâ”€â”€ meta.json          # Run configuration
â”‚       â”œâ”€â”€ control.json       # Control signals (pause, stop, etc.)
â”‚       â”œâ”€â”€ heartbeat          # Last heartbeat timestamp
â”‚       â”œâ”€â”€ progress.json      # Progress updates from wrapper
â”‚       â”œâ”€â”€ failure.json       # Last failure info
â”‚       â”œâ”€â”€ final.json         # Completion marker
â”‚       â”œâ”€â”€ ended.json         # Termination reason
â”‚       â”œâ”€â”€ events.log         # Run event history
â”‚       â”œâ”€â”€ slurm.out          # SLURM stdout
â”‚       â”œâ”€â”€ slurm.err          # SLURM stderr
â”‚       â””â”€â”€ badnode_events.log # Node failure history
â”œâ”€â”€ blacklist.json             # Global node blacklist
â”œâ”€â”€ remotes.json               # Remote cluster configs
â”œâ”€â”€ locks/                     # Per-run locks
â””â”€â”€ daemon.pid                 # Daemon PID file
```

## Tests

```bash
python -m unittest discover -s tests -v
```

## Contributing

Contributions are welcome! Please feel free to submit issues and pull requests.

## License

MIT License â€” see [LICENSE](LICENSE) for details.

---

<div align="center">

**[â¬† Back to top](#-shepherd)**

<sub>Built with frustration from too many failed training runs ğŸ”¥</sub>

</div>
